{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import BayesianLayers\n",
    "from compression import compute_compression_rate, compute_reduced_weights\n",
    "from utils import visualize_pixel_importance, generate_gif, visualise_weights\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "from scipy import linalg\n",
    "import matplotlib as mpl\n",
    "!pip install pyomo\n",
    "!pip install pypsa\n",
    "!pip install pandapower\n",
    "import pypsa\n",
    "import pandapower as pp\n",
    "import pandapower.networks as pn\n",
    "from pandapower.estimation import estimate\n",
    "import random as rand\n",
    "import pandas as pd\n",
    "from sklearn import mixture\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "sns.set()\n",
    "rand.seed(2020)\n",
    "tf.random.set_seed(2020)\n",
    "torch.manual_seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import extract_consumption_values\n",
    "from bic import bic_criterion_gmm\n",
    "from define_network import define_network\n",
    "from generate_samples import generate_samples\n",
    "from compress_model import compress_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obs: case_14s and case_57 correspond to the IEEE standard test cases. case_3 corresponds to the 3-bus minimal example network on PyPSA, while opf_storage_hvdc corresponds to the PyPSA opf_storage_hvdc example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_case = \"case_3\" # Select ['case_14s', 'case_57', 'case_3', 'opf-storage-hvdc']\n",
    "N = 60000.  # number of data points in the training set\n",
    "n_of_network_samples = 100\n",
    "n_of_gaussians = 5\n",
    "train_frac = 0.8\n",
    "percent_of_measurements = 0.4\n",
    "estimation_method = 'bad_data_removal' # Select ['standard','bad_data_removal']\n",
    "layer_proportions = [20,10]\n",
    "net_type = 'case_3' # Select ['pp','case_3', 'opf-storage-hvdc'] pp is for pandapower cases 14s and 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./plots/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"unzip opf-storage-hvdc.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Solar and Normal data separately with Gaussian Mixture\n",
    "#### Initially, a GMM with 3 components will be fitted, like in the original paper. Later, other pdfs will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"unzip electricity_data.zip\")\n",
    "base_path = \"./electricity_data/\"\n",
    "\n",
    "train_data_path = base_path + \"2011-2012 Solar home electricity data v2.csv\"\n",
    "test_data_path = base_path + \"2012-2013 Solar home electricity data v2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path, skiprows=1)\n",
    "test_data = pd.read_csv(test_data_path, skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GC = General Consumption for electricity supplied all the time (primary tariff, either inclining block or time of use rates), excluding solar generation and controlled load supply \n",
    "### CL = Controlled Load Consumption (Off peak 1 or 2 tariffs)\n",
    "### GG = Gross Generation for electricity generated by the solar system with a gross metering configuration, measured separately to household loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_general, train_data_solar = extract_consumption_values(train_data)\n",
    "test_data_general, test_data_solar = extract_consumption_values(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = np.reshape(train_data_general.values, (train_data_general.shape[0]*train_data_general.shape[1],1))\n",
    "test_values = np.reshape(test_data_general.values, (test_data_general.shape[0]*test_data_general.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_criterion_gmm(10, train_values, test_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_of_components=5 was initially chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = mixture.GaussianMixture(n_components=n_of_gaussians)\n",
    "#fit it to the data\n",
    "gmm.fit(train_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solar consumption data was either 0 or NaN. We will only fit GMM on the General Consumption values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating samples from Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = define_network(net_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injection_values, network_state_samples, measurement_vector = generate_samples(net_case,n_of_network_samples,net, percent_of_measurements, estimation_method, gmm, net_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(layer_proportions[0]*measurement_vector.shape[1],input_dim=measurement_vector.shape[1]),\n",
    "    tf.keras.layers.Dense(layer_proportions[1]*measurement_vector.shape[1], input_dim=layer_proportions[0]*measurement_vector.shape[1]),\n",
    "    tf.keras.layers.Dense(network_state_samples.shape[1], input_dim=layer_proportions[1]*measurement_vector.shape[1]),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='mse', # keras.losses.mean_squared_error\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=0.2),\n",
    ")\n",
    "\n",
    "print(\"Training ... With default parameters, this takes less than 10 seconds.\")\n",
    "training_history = model.fit(\n",
    "    injection_values[:int(measurement_vector.shape[0]*train_frac)], # input\n",
    "    network_state_samples[:int(measurement_vector.shape[0]*train_frac)], # output\n",
    "    batch_size=int(measurement_vector.shape[0]*train_frac),\n",
    "    verbose=0, # Suppress chatty output; use Tensorboard instead\n",
    "    epochs=50,\n",
    "    validation_data=(injection_values[int(measurement_vector.shape[0]*train_frac):], network_state_samples[int(measurement_vector.shape[0]*train_frac):]),\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "\n",
    "print(\"Average test loss: \", np.average(training_history.history['loss']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(training_history.history['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model.predict(injection_values[-10:]),network_state_samples[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below runs the Bayesian Compression step. The variable \"Test loss\" is the MSE reported in the draft, while the compression factor is the one based on the weight uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    class FLAGS:\n",
    "        epochs=50\n",
    "        batchsize=100\n",
    "        thresholds=[-2.8, -3., -5.]\n",
    "        \n",
    "    FLAGS.cuda = torch.cuda.is_available()  # check if we can put the net on the GPU\n",
    "\n",
    "    compress_model(injection_values, measurement_vector, train_frac, network_state_samples, layer_proportions, N, FLAGS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
