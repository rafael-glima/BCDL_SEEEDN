{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import BayesianLayers\n",
    "from compression import compute_compression_rate, compute_reduced_weights\n",
    "from utils import visualize_pixel_importance, generate_gif, visualise_weights\n",
    "\n",
    "import itertools\n",
    "from scipy import linalg\n",
    "import matplotlib as mpl\n",
    "import pandapower as pp\n",
    "import pandapower.networks as pn\n",
    "from pandapower.estimation import estimate\n",
    "import random as rand\n",
    "import pandas as pd\n",
    "from sklearn import mixture\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "sns.set()\n",
    "rand.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 60000.  # number of data points in the training set\n",
    "n_of_network_samples = 100\n",
    "n_of_gaussians = 5\n",
    "train_frac = 0.8\n",
    "percent_of_measurements = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Solar and Normal data separately with Gaussian Mixture\n",
    "#### Initially, a GMM with 3 components will be fitted, like in the original paper. Later, other pdfs will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/electricity_data/\"\n",
    "\n",
    "train_data_path = base_path + \"2011-2012 Solar home electricity data v2.csv\"\n",
    "test_data_path = base_path + \"2012-2013 Solar home electricity data v2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path, skiprows=1)\n",
    "test_data = pd.read_csv(test_data_path, skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GC = General Consumption for electricity supplied all the time (primary tariff, either inclining block or time of use rates), excluding solar generation and controlled load supply \n",
    "### CL = Controlled Load Consumption (Off peak 1 or 2 tariffs)\n",
    "### GG = Gross Generation for electricity generated by the solar system with a gross metering configuration, measured separately to household loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_consumption_values(data_df):\n",
    "    \n",
    "    selected_columns = [col for col in data_df.columns if \":\" in col]\n",
    "\n",
    "    data_df_general = data_df[data_df['Consumption Category'] == 'GC']\n",
    "    \n",
    "    data_df_solar = data_df[data_df['Consumption Category'] == 'GG']\n",
    "    \n",
    "    return data_df_general[selected_columns], data_df_solar[selected_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_general, train_data_solar = extract_consumption_values(train_data)\n",
    "test_data_general, test_data_solar = extract_consumption_values(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = np.reshape(train_data_general.values, (train_data_general.shape[0]*train_data_general.shape[1],1))\n",
    "test_values = np.reshape(test_data_general.values, (test_data_general.shape[0]*test_data_general.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit a Dirichlet process Gaussian mixture using five components\n",
    "# dpgmm = mixture.BayesianGaussianMixture(n_components=n_of_gaussians,\n",
    "#                                         covariance_type='full').fit(train_values)\n",
    "# # plot_results(train_values, dpgmm.predict(train_values), dpgmm.means_, dpgmm.covariances_, 1,\n",
    "# #              'Bayesian Gaussian Mixture with a Dirichlet process prior (train)')\n",
    "# plt.scatter(dpgmm.predict(test_values),test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot_results(test_values, dpgmm.predict(test_values), dpgmm.means_, dpgmm.covariances_, 1,\n",
    "# #              'Bayesian Gaussian Mixture with a Dirichlet process prior (test)')\n",
    "# plt.scatter(gmm.predict(test_values),test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code based on: https://stackoverflow.com/questions/39920862/model-selection-for-gaussianmixture-by-using-gridsearch \n",
    "bic = np.zeros(9)\n",
    "n = np.arange(1,10)\n",
    "models = []\n",
    "#loop through each number of Gaussians and compute the BIC, and save the model\n",
    "for i,j in enumerate(n):\n",
    "    #create mixture model with j components\n",
    "    gmm = mixture.GaussianMixture(n_components=j)\n",
    "    #fit it to the data\n",
    "    gmm.fit(train_values)\n",
    "    #compute the BIC for this model\n",
    "    bic[i] = gmm.bic(test_values)\n",
    "    #add the best-fit model with j components to the list of models\n",
    "    models.append(gmm)\n",
    "    \n",
    "plt.plot(n,bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_of_components=5 was initially chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = mixture.GaussianMixture(n_components=n_of_gaussians)\n",
    "#fit it to the data\n",
    "gmm.fit(train_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solar consumption data was either 0 or NaN. We will only fit GMM on the General Consumption values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating samples from Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "# net = pn.case14()\n",
    "net = pn.case57()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injection_values, network_state_samples, measurement_vector = np.zeros((n_of_network_samples,net.bus.shape[0])),\\\n",
    "np.zeros((n_of_network_samples,net.bus.shape[0])), np.zeros((n_of_network_samples,int(net.bus.shape[0]*percent_of_measurements)))\n",
    "\n",
    "measurement_indices = rand.sample(range(net.bus.shape[0]),int(net.bus.shape[0]*percent_of_measurements))\n",
    "\n",
    "print(range(net.bus.shape[0]))\n",
    "\n",
    "print(measurement_indices)\n",
    "\n",
    "for i in range(n_of_network_samples):\n",
    "\n",
    "    net = pn.case57()\n",
    "    \n",
    "    injection_values_per_iter = -2e-3*np.squeeze(gmm.sample(n_samples=net.bus.shape[0])[0])\n",
    "    \n",
    "    injection_values[i,:] = injection_values_per_iter\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    # Need to figure out to to get value parameter for line current measurements, since the data only show max voltage.\n",
    "    \n",
    "    # For now, working with voltage measurements\n",
    "    \n",
    "    measurements = np.zeros((net.bus.shape[0],))\n",
    "    \n",
    "    for j in range(net.bus.shape[0]):\n",
    "        \n",
    "        ref_bus_voltage = net.bus.vn_kv[j]\n",
    "        \n",
    "        measurement_value = ref_bus_voltage + rand.normalvariate(0,0.1*ref_bus_voltage)\n",
    "            \n",
    "        pp.create_measurement(net=net, meas_type='v', element_type='bus', value=ref_bus_voltage, std_dev=0.1*ref_bus_voltage, element=j, side=None, check_existing=True, index=None, name=None)\n",
    " \n",
    "        pp.create_measurement(net=net, meas_type='p', element_type='bus', value=injection_values_per_iter[j], std_dev=0.1*ref_bus_voltage, element=j, side=None, check_existing=True, index=None, name=None)\n",
    "\n",
    "        pp.create_sgen(net=net, bus=j, p_mw=injection_values_per_iter[j])\n",
    "        \n",
    "        \n",
    "    pp.runpp(net)\n",
    "    \n",
    "    success = pp.estimation.estimate(net, init=\"flat\")\n",
    "    V, delta = net.res_bus_est.vm_pu, net.res_bus_est.va_degree\n",
    "\n",
    "    #print(\"Z: \", net.res_bus.vm_pu.values)\n",
    "    \n",
    "    network_state_samples[i,:] = net.res_bus.vm_pu.values\n",
    "    \n",
    "    measurement_vector[i,:] = measurements[measurement_indices] #net.res_bus_est.vm_pu.values[measurement_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(2*measurement_vector.shape[1],input_dim=measurement_vector.shape[1]),\n",
    "    tf.keras.layers.Dense(measurement_vector.shape[1], input_dim=2*measurement_vector.shape[1]),\n",
    "    tf.keras.layers.Dense(network_state_samples.shape[1], input_dim=measurement_vector.shape[1]),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='mse', # keras.losses.mean_squared_error\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=0.2),\n",
    ")\n",
    "\n",
    "print(\"Training ... With default parameters, this takes less than 10 seconds.\")\n",
    "training_history = model.fit(\n",
    "    injection_values[:int(measurement_vector.shape[0]*train_frac)], # input\n",
    "    network_state_samples[:int(measurement_vector.shape[0]*train_frac)], # output\n",
    "    batch_size=int(measurement_vector.shape[0]*train_frac),\n",
    "    verbose=0, # Suppress chatty output; use Tensorboard instead\n",
    "    epochs=50,\n",
    "    validation_data=(injection_values[int(measurement_vector.shape[0]*train_frac):], network_state_samples[int(measurement_vector.shape[0]*train_frac):]),\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "\n",
    "print(\"Average test loss: \", np.average(training_history.history['loss']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(training_history.history['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model.predict(injection_values[-10:]),network_state_samples[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "This portion of the code is based in the 'example.py' file of: \n",
    "Karen Ullrich, Christos Louizos, Oct 2017\n",
    "\"\"\"\n",
    "\n",
    "def main(injection_values, measurement_vector, train_frac, network_state_samples, FLAGS):\n",
    "    # import data\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if FLAGS.cuda else {}\n",
    "    \n",
    "    train_tensor_x = torch.Tensor(measurement_vector[:int(measurement_vector.shape[0]*train_frac)]).unsqueeze(1) # transform to torch tensor\n",
    "    train_tensor_y = torch.Tensor(network_state_samples[:int(measurement_vector.shape[0]*train_frac)])#.unsqueeze(1)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_tensor_x,train_tensor_y) # create your datset\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=FLAGS.batchsize, shuffle=True, **kwargs) # create your dataloader\n",
    "\n",
    "    print(train_tensor_x.shape,train_tensor_y.shape)\n",
    "    \n",
    "    \n",
    "    test_tensor_x = torch.Tensor(measurement_vector[int(measurement_vector.shape[0]*train_frac):]).unsqueeze(1) # transform to torch tensor\n",
    "    test_tensor_y = torch.Tensor(network_state_samples[int(measurement_vector.shape[0]*train_frac):])#.unsqueeze(1)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_tensor_x,test_tensor_y) # create your datset\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=FLAGS.batchsize, shuffle=True, **kwargs) # create your dataloader\n",
    "\n",
    "    # for later analysis we take some sample digits\n",
    "    mask = 255. * (np.ones((1, 1, injection_values[0].shape[0])))\n",
    "\n",
    "    # build a simple MLP\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            # activation\n",
    "            self.relu = nn.ReLU()\n",
    "            # layers\n",
    "            self.fc1 = BayesianLayers.LinearGroupNJ(measurement_vector.shape[1], 10*measurement_vector.shape[1], clip_var=0.04, cuda=FLAGS.cuda)\n",
    "            self.fc2 = BayesianLayers.LinearGroupNJ(10*measurement_vector.shape[1],5*measurement_vector.shape[1], cuda=FLAGS.cuda)\n",
    "            self.fc3 = BayesianLayers.LinearGroupNJ(5*measurement_vector.shape[1],network_state_samples.shape[1], cuda=FLAGS.cuda)\n",
    "            # layers including kl_divergence\n",
    "            self.kl_list = [self.fc1, self.fc2, self.fc3]\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, measurement_vector.shape[1])#28 * 28)\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.relu(self.fc2(x))\n",
    "            return self.fc3(x)\n",
    "\n",
    "        def get_masks(self,thresholds):\n",
    "            weight_masks = []\n",
    "            mask = None\n",
    "            for i, (layer, threshold) in enumerate(zip(self.kl_list, thresholds)):\n",
    "                # compute dropout mask\n",
    "                if mask is None:\n",
    "                    log_alpha = layer.get_log_dropout_rates().cpu().data.numpy()\n",
    "                    mask = log_alpha < threshold\n",
    "                else:\n",
    "                    mask = np.copy(next_mask)\n",
    "                try:\n",
    "                    log_alpha = layers[i + 1].get_log_dropout_rates().cpu().data.numpy()\n",
    "                    next_mask = log_alpha < thresholds[i + 1]\n",
    "                except:\n",
    "                    # must be the last mask\n",
    "                    next_mask = np.ones(network_state_samples.shape[1])\n",
    "\n",
    "                weight_mask = np.expand_dims(mask, axis=0) * np.expand_dims(next_mask, axis=1)\n",
    "                weight_masks.append(weight_mask.astype(np.float))\n",
    "            return weight_masks\n",
    "\n",
    "        def kl_divergence(self):\n",
    "            KLD = 0\n",
    "            for layer in self.kl_list:\n",
    "                KLD += layer.kl_divergence()\n",
    "            return KLD\n",
    "\n",
    "    # init model\n",
    "    model = Net()\n",
    "    if FLAGS.cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    # init optimizer\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # we optimize the variational lower bound scaled by the number of data\n",
    "    # points (so we can keep our intuitions about hyper-params such as the learning rate)\n",
    "    discrimination_loss = nn.functional.mse_loss #cross_entropy\n",
    "\n",
    "    def objective(output, target, kl_divergence):\n",
    "        discrimination_error = discrimination_loss(output, target)\n",
    "        variational_bound = discrimination_error + kl_divergence / N\n",
    "        if FLAGS.cuda:\n",
    "            variational_bound = variational_bound.cuda()\n",
    "        return variational_bound\n",
    "\n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if FLAGS.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            #print(\"output: \", output.shape,\"target: \", target.squeeze().shape)\n",
    "            loss = objective(output, target, model.kl_divergence())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # clip the variances after each step\n",
    "            for layer in model.kl_list:\n",
    "                layer.clip_variances()\n",
    "        print('Epoch: {} \\tTrain loss: {:.8f} \\t'.format(\n",
    "            epoch, loss.data))\n",
    "\n",
    "    def test():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            if FLAGS.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            output = model(data)\n",
    "            test_loss += discrimination_loss(output, target, size_average=False).data\n",
    "            pred = output.data#.max(1, keepdim=True)[1]\n",
    "            #print(\"pred: \", pred.shape)\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('Test loss: {:.8f}, Accuracy: {}/{} ({:.8f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    # train the model and save some visualisations on the way\n",
    "    for epoch in range(1, FLAGS.epochs + 1):\n",
    "        train(epoch)\n",
    "        test()\n",
    "        # visualizations\n",
    "        weight_mus = [model.fc1.weight_mu, model.fc2.weight_mu]\n",
    "        log_alphas = [model.fc1.get_log_dropout_rates(), model.fc2.get_log_dropout_rates(),\n",
    "                      model.fc3.get_log_dropout_rates()]\n",
    "        visualise_weights(weight_mus, log_alphas, epoch=epoch)\n",
    "        log_alpha = model.fc1.get_log_dropout_rates().cpu().data.numpy()\n",
    "        #visualize_pixel_importance(images, log_alpha=log_alpha, epoch=str(epoch))\n",
    "\n",
    "#     generate_gif(save='pixel', epochs=FLAGS.epochs)\n",
    "#     generate_gif(save='weight0_e', epochs=FLAGS.epochs)\n",
    "#     generate_gif(save='weight1_e', epochs=FLAGS.epochs)\n",
    "\n",
    "    # compute compression rate and new model accuracy\n",
    "    layers = [model.fc1, model.fc2, model.fc3]\n",
    "    thresholds = FLAGS.thresholds\n",
    "    #print(model.get_masks(thresholds))\n",
    "    compute_compression_rate(layers, model.get_masks(thresholds))\n",
    "\n",
    "    print(\"Test error after with reduced bit precision:\")\n",
    "\n",
    "    weights = compute_reduced_weights(layers, model.get_masks(thresholds))\n",
    "    for layer, weight in zip(layers, weights):\n",
    "        if FLAGS.cuda:\n",
    "            layer.post_weight_mu.data = torch.Tensor(weight).cuda()\n",
    "        else:\n",
    "            layer.post_weight_mu.data = torch.Tensor(weight)\n",
    "    for layer in layers: layer.deterministic = True\n",
    "    test()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    class FLAGS:\n",
    "        epochs=50\n",
    "        batchsize=100\n",
    "        thresholds=[-2.8, -3., -5.]\n",
    "        \n",
    "    FLAGS.cuda = torch.cuda.is_available()  # check if we can put the net on the GPU\n",
    "\n",
    "    main(injection_values, measurement_vector, train_frac, network_state_samples, FLAGS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
